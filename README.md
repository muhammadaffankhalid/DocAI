# ğŸ§  DocAI

DocAI is an AI-powered document assistant that lets you upload a document (PDF or text) and ask natural language questions about it. The answers are generated by a locally running language model (Mistral 7B). No internet-required inference â€” 100% offline and private.

---

## ğŸš€ Features

* Upload PDF or TXT files
* Ask questions about the uploaded document
* Answers generated using Mistral-7B locally via `llama.cpp`
* Smart context extraction for faster and relevant responses
* Simple front-end with vanilla CSS (no framework required)
* Works offline â€” no data is sent to the cloud

---

## ğŸ§± Project Structure

```
docai/
â”œâ”€â”€ back-end/        # FastAPI backend and model loader
â”œâ”€â”€ front-end/       # React frontend
â”œâ”€â”€ README.md        # You're here
â””â”€â”€ .gitignore       # Ignores model files, node_modules, etc.
```

---

## âš™ï¸ Requirements

### ğŸ”§ Backend

* Python 3.9+
* FastAPI
* `llama-cpp-python`
* `PyMuPDF` (fitz)
* `PyPDF2`

### ğŸ’» Frontend

* Node.js 18+
* npm or yarn

---

## ğŸ§ª Local Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/DocAI.git
cd DocAI
```

---

### 2. Backend Setup (FastAPI + Llama)

> The model **must be downloaded separately** due to its size.

```bash
cd back-end
python -m venv venv
source venv/bin/activate

pip install -r requirements.txt
```

Place your **Mistral model file** (e.g. `mistral-7b-instruct-v0.1.Q2_K.gguf`) inside:

```
back-end/models/
```

Then run the backend:

```bash
uvicorn docai_backend:app --host 127.0.0.1 --port 8081
```

> âœ… Visit `http://127.0.0.1:8081` to check if backend is running.

---

### 3. Frontend Setup (React)

```bash
cd ../front-end
npm install
npm run dev
```

Visit the frontend in your browser:

```
http://localhost:5173
```

---

## ğŸ“ How to Use

1. Start the backend (`uvicorn`)
2. Start the frontend (`npm run dev`)
3. Upload a PDF or text file
4. Type in your question
5. Click "Ask" â€” and wait for your answer!

---

## ğŸ§  Model Used

We use **Mistral 7B Instruct** in `GGUF` format for fast, local inference using [`llama.cpp`](https://github.com/ggerganov/llama.cpp).

Download the model from:
ğŸ‘‰ [https://huggingface.co/TheBloke/Mistral-7B-Instruct-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-GGUF)

Put the model file here:

```
back-end/models/mistral-7b-instruct-v0.1.Q2_K.gguf
```

---

## ğŸ“ .gitignore (already configured)

This project includes a `.gitignore` that prevents you from pushing:

* Large model files (`.gguf`)
* Node modules
* Python cache files
* Environment variables

---





## ğŸ“„ License

[MIT](LICENSE)

---

## ğŸ§“â€â™‚ï¸ Author

Made with ğŸ’¡ by **Muhammad Affan Khalid**
[GitHub Profile â†’](https://github.com/muhammadaffankhalid)
