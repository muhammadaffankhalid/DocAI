# 🧠 DocAI

DocAI is an AI-powered document assistant that lets you upload a document (PDF or text) and ask natural language questions about it. The answers are generated by a locally running language model (Mistral 7B). No internet-required inference — 100% offline and private.

---

## 🚀 Features

* Upload PDF or TXT files
* Ask questions about the uploaded document
* Answers generated using Mistral-7B locally via `llama.cpp`
* Smart context extraction for faster and relevant responses
* Simple front-end with vanilla CSS (no framework required)
* Works offline — no data is sent to the cloud

---

## 🧱 Project Structure

```
docai/
├── back-end/        # FastAPI backend and model loader
├── front-end/       # React frontend
├── README.md        # You're here
└── .gitignore       # Ignores model files, node_modules, etc.
```

---

## ⚙️ Requirements

### 🔧 Backend

* Python 3.9+
* FastAPI
* `llama-cpp-python`
* `PyMuPDF` (fitz)
* `PyPDF2`

### 💻 Frontend

* Node.js 18+
* npm or yarn

---

## 🧪 Local Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/DocAI.git
cd DocAI
```

---

### 2. Backend Setup (FastAPI + Llama)

> The model **must be downloaded separately** due to its size.

```bash
cd back-end
python -m venv venv
source venv/bin/activate

pip install -r requirements.txt
```

Place your **Mistral model file** (e.g. `mistral-7b-instruct-v0.1.Q2_K.gguf`) inside:

```
back-end/models/
```

Then run the backend:

```bash
uvicorn docai_backend:app --host 127.0.0.1 --port 8081
```

> ✅ Visit `http://127.0.0.1:8081` to check if backend is running.

---

### 3. Frontend Setup (React)

```bash
cd ../front-end
npm install
npm run dev
```

Visit the frontend in your browser:

```
http://localhost:5173
```

---

## 📝 How to Use

1. Start the backend (`uvicorn`)
2. Start the frontend (`npm run dev`)
3. Upload a PDF or text file
4. Type in your question
5. Click "Ask" — and wait for your answer!

---

## 🧠 Model Used

We use **Mistral 7B Instruct** in `GGUF` format for fast, local inference using [`llama.cpp`](https://github.com/ggerganov/llama.cpp).

Download the model from:
👉 [https://huggingface.co/TheBloke/Mistral-7B-Instruct-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-GGUF)

Put the model file here:

```
back-end/models/mistral-7b-instruct-v0.1.Q2_K.gguf
```

---

## 📁 .gitignore (already configured)

This project includes a `.gitignore` that prevents you from pushing:

* Large model files (`.gguf`)
* Node modules
* Python cache files
* Environment variables

---





## 📄 License

[MIT](LICENSE)

---

## 🧓‍♂️ Author

Made with 💡 by **Muhammad Affan Khalid**
[GitHub Profile →](https://github.com/muhammadaffankhalid)
